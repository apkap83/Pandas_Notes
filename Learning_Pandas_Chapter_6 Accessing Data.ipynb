{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Accessing Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Reading and writing pandas data from files</li>\n",
    "<li>Working with data in CSV, JSON, HTML, Excel, and HDF5 formats</li>\n",
    "<li>Accessing data on the web and in the cloud</li>\n",
    "<li>Reading and writing from/to SQL databases</li>\n",
    "<li>Reading data from remote web data services</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set some pandas options for controlling output\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading a CSV file into a DataFrame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date   Open   High    Low  Close   Volume  Adj Close\n",
       "0  2014-07-21  83.46  83.53  81.81  81.93  2359300      81.93\n",
       "1  2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35\n",
       "2  2014-07-17  84.35  84.63  83.33  83.63  1974000      83.63\n",
       "3  2014-07-16  83.77  84.91  83.66  84.91  1755600      84.91"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in msft.csv into a DataFrame\n",
    "msft = pd.read_csv(\"data/msft.csv\")\n",
    "msft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was easy! pandas has realized that the first line of the file contains the names of\n",
    "the columns and bulk read in the data to DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Specifying the index column when reading a CSV file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Open   High    Low  Close   Volume  Adj Close\n",
       "Date                                                      \n",
       "2014-07-21  83.46  83.53  81.81  81.93  2359300      81.93\n",
       "2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35\n",
       "2014-07-17  84.35  84.63  83.33  83.63  1974000      83.63\n",
       "2014-07-16  83.77  84.91  83.66  84.91  1755600      84.91"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use column 0 as the index\n",
    "msft = pd.read_csv(\"data/msft.csv\", index_col=0)\n",
    "msft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft['MyDate'] = msft.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Open   High    Low  Close   Volume  Adj Close      MyDate\n",
       "Date                                                                  \n",
       "2014-07-21  83.46  83.53  81.81  81.93  2359300      81.93  2014-07-21\n",
       "2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35  2014-07-18\n",
       "2014-07-17  84.35  84.63  83.33  83.63  1974000      83.63  2014-07-17\n",
       "2014-07-16  83.77  84.91  83.66  84.91  1755600      84.91  2014-07-16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Volume         int64\n",
       "Adj Close    float64\n",
       "MyDate        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the types of the columns in this DataFrame\n",
    "msft.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To force the types of columns, use the dtypes parameter of pd.read_csv(). The following\n",
    "forces the Volume column to also be float64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          object\n",
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Volume       float64\n",
       "Adj Close    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify that the Volume column should be a float64\n",
    "msft = pd.read_csv(\"data/msft.csv\", dtype = { 'Volume' : np.float})\n",
    "msft.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify the column names at the time of reading the data using the\n",
    "names parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             open   high    low  close   volume  adjclose\n",
       "2014-07-21  83.46  83.53  81.81  81.93  2359300     81.93\n",
       "2014-07-18  83.30  83.40  82.52  83.35  4020800     83.35\n",
       "2014-07-17  84.35  84.63  83.33  83.63  1974000     83.63\n",
       "2014-07-16  83.77  84.91  83.66  84.91  1755600     84.91"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify a new set of names for the columns\n",
    "# all lower case, remove space in Adj Close\n",
    "# also, header=0 skips the header row\n",
    "df = pd.read_csv(\"data/msft.csv\", header=0, names=['open', 'high', 'low',\n",
    "'close', 'volume', 'adjclose'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             open   high    low  close   volume  adjclose\n",
       "2014-07-21  83.46  83.53  81.81  81.93  2359300     81.93\n",
       "2014-07-18  83.30  83.40  82.52  83.35  4020800     83.35\n",
       "2014-07-17  84.35  84.63  83.33  83.63  1974000     83.63\n",
       "2014-07-16  83.77  84.91  83.66  84.91  1755600     84.91"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify a new set of names for the columns\n",
    "# all lower case, remove space in Adj Close\n",
    "# also, header=0 skips the header row\n",
    "df = pd.read_csv(\"data/msft.csv\", header=0, names=['open', 'high', 'low',\n",
    "'close', 'volume', 'adjclose'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because we specified the names of the columns, we need to skip over the column\n",
    "namesâ€™ row in the file, which was performed with header=0. If not, pandas will assume\n",
    "the first row is part of the data, which will cause some issues later in processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Specify specific columns to load</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify which columns to load when reading the file. This can be\n",
    "useful if you have a lot of columns in the file and some are of no interest to your analysis\n",
    "and you want to save the time and memory required to read and store them. Specifying\n",
    "which columns to read is accomplished with the usecols parameter, which can be passed\n",
    "a list of column names or column offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Close\n",
       "Date             \n",
       "2014-07-21  81.93\n",
       "2014-07-18  83.35\n",
       "2014-07-17  83.63\n",
       "2014-07-16  84.91"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data only in the Date and Close columns\n",
    "# and index by the Date column\n",
    "df2 = pd.read_csv(\"data/msft.csv\", usecols=['Date', 'Close'], index_col=['Date'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving DataFrame to a csv file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Close\n",
       "Date             \n",
       "2014-07-21  81.93\n",
       "2014-07-18  83.35\n",
       "2014-07-17  83.63\n",
       "2014-07-16  84.91"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save df2 to a new csv file\n",
    "# also specify naming the index as date\n",
    "df2.to_csv(\"data/msft_modified.csv\", index_label='date')\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to tell the method that the index label should be saved with a column\n",
    "name of date using index_label=date. Otherwise, the index does not have a name\n",
    "added to the first row of the file, which makes it difficult to read back properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date,Close\r\n",
      "2014-07-21,81.93\r\n",
      "2014-07-18,83.35\r\n",
      "2014-07-17,83.63\r\n",
      "2014-07-16,84.91\r\n"
     ]
    }
   ],
   "source": [
    "# view the start of the file just saved\n",
    "!head data/msft_modified.csv # Linux or osx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>General field-delimited data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Open   High    Low  Close   Volume  Adj Close\n",
       "Date                                                      \n",
       "2014-07-21  83.46  83.53  81.81  81.93  2359300      81.93\n",
       "2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35\n",
       "2014-07-17  84.35  84.63  83.33  83.63  1974000      83.63\n",
       "2014-07-16  83.77  84.91  83.66  84.91  1755600      84.91"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use read_table with sep=',' to read a CSV\n",
    "df = pd.read_table(\"data/msft.csv\", sep=',', index_col=['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pipe delimited\n",
    "df.to_csv(\"data/msft_piped.txt\", sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date|Open|High|Low|Close|Volume|Adj Close\r\n",
      "2014-07-21|83.46|83.53|81.81|81.93|2359300|81.93\r\n",
      "2014-07-18|83.3|83.4|82.52|83.35|4020800|83.35\r\n",
      "2014-07-17|84.35|84.63|83.33|83.63|1974000|83.63\r\n",
      "2014-07-16|83.77|84.91|83.66|84.91|1755600|84.91\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 data/msft_piped.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Handling noise rows in field-delimited data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip unwanted rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>#!head data\\msft2.csv\n",
    "This is fun because the data does not start on the first line\n",
    "Date,Open,High,Low,Close,Volume,Adj Close\n",
    "\n",
    "And there is space between the header row and data\n",
    "\n",
    "2014-07-21,83.46,83.53,81.81,81.93,2359300,81.93\n",
    "2014-07-18,83.30,83.40,82.52,83.35,4020800,83.35\n",
    "2014-07-17,84.35,84.63,83.33,83.63,1974000,83.63\n",
    "2014-07-16,83.77,84.91,83.66,84.91,1755600,84.91\n",
    "2014-07-15,84.30,84.38,83.20,83.58,1874700,83.58\n",
    "2014-07-14,83.66,84.64,83.11,84.40,1432100,84.40</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date   Open   High    Low  Close   Volume  Adj Close\n",
       "0  2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35\n",
       "1  2014-07-17  84.35  84.63  83.33  83.63  1974000      83.63\n",
       "2  2014-07-16  83.77  84.91  83.66  84.91  1755600      84.91\n",
       "3  2014-07-15  84.30  84.38  83.20  83.58  1874700      83.58\n",
       "4  2014-07-14  83.66  84.64  83.11  84.40  1432100      84.40"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read, but skip rows 0, 2 and 3\n",
    "df = pd.read_csv(\"data/msft2.csv\", skiprows=[0, 2, 3])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common situation is where a file has content at the end of the file, which should\n",
    "be ignored to prevent an error, such as the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "# another messy file, with the mess at the end\n",
    "!cat data/msft_with_footer.csv # osx or Linux\n",
    "Date,Open,High,Low,Close,Volume,Adj Close\n",
    "2014-07-21,83.46,83.53,81.81,81.93,2359300,81.93\n",
    "2014-07-18,83.30,83.40,82.52,83.35,4020800,83.35\n",
    "Uh oh, there is stuff at the end.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will cause an exception during reading, but it can be handled using the skip_footer\n",
    "parameter, which specifies how many lines at the end of the file to ignore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         Date   Open   High    Low  Close   Volume  Adj Close\n",
       "0  2014-07-21  83.46  83.53  81.81  81.93  2359300      81.93\n",
       "1  2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skip only two lines at the end\n",
    "df = pd.read_csv(\"data/msft_with_footer.csv\", skipfooter=2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date   Open   High    Low  Close   Volume  Adj Close\n",
       "0  2014-07-21  83.46  83.53  81.81  81.93  2359300      81.93\n",
       "1  2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skip only two lines at the end\n",
    "df = pd.read_csv(\"data/msft_with_footer.csv\", skipfooter=2, engine='python')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the file is large and you only want to read the first few rows, as you only want the\n",
    "data at the start of the file and do not want to read it all into the memory. This can be\n",
    "handled with the nrows parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date   Open   High    Low  Close   Volume  Adj Close\n",
       "0  2014-07-21  83.46  83.53  81.81  81.93  2359300      81.93\n",
       "1  2014-07-18  83.30  83.40  82.52  83.35  4020800      83.35\n",
       "2  2014-07-17  84.35  84.63  83.33  83.63  1974000      83.63"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only process the first three rows\n",
    "pd.read_csv(\"data/msft.csv\", nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example skips 100 rows and then reads in the next\n",
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [open, high, low, close, vol, adjclose]\n",
       "Index: []"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skip 100 lines, then only process the next five\n",
    "pd.read_csv(\"data/msft.csv\", skiprows=100, nrows=5,header=0,\n",
    "            names=['open', 'high', 'low', 'close', 'vol','adjclose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>\n",
    "<p>Note that the preceding example also skipped reading the header line, so it was necessary\n",
    "to inform the process to not look for a header and use the specified names.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading and writing data in an Excel format</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas supports reading data in Excel 2003 and newer formats using the\n",
    "pd.read_excel() function or via the ExcelFile class. Internally, both techniques use\n",
    "either the XLRD or OpenPyXL packages, so you will need to ensure that either is installed\n",
    "first in your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close    Volume  Adj Close\n",
       "0  7/21/2014  94.99  95.00  93.72  93.94  38887700      93.94\n",
       "1  7/18/2014  93.62  94.74  93.02  94.43  49898600      94.43\n",
       "2  7/17/2014  95.03  95.28  92.57  93.09  57152000      93.09\n",
       "3  7/16/2014  96.97  97.10  94.74  94.78  53396300      94.78\n",
       "4  7/15/2014  96.80  96.85  95.03  95.32  45477900      95.32"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read excel file\n",
    "# only reads first sheet (aapl in this case)\n",
    "df = pd.read_excel(\"data/stocks.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has read only content from the first worksheet in the Excel file (the aapl worksheet)\n",
    "and used the contents of the first row as column names. To read the other worksheet, you\n",
    "can pass the name of the worksheet using the sheetname parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date    Open   High    Low  Close    Volume  Adj Close\n",
       "0  7/21/2014  150.99  95.00  93.72  93.94  38887700      93.94\n",
       "1  7/18/2014   93.62  94.74  93.02  94.43  49898600      94.43\n",
       "2  7/17/2014   95.03  95.28  92.57  93.09  57152000      93.09\n",
       "3  7/16/2014   96.97  97.10  94.74  94.78  53396300      94.78\n",
       "4  7/15/2014   96.80  96.85  95.03  95.32  45477900      95.32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from the aapl worksheet\n",
    "aapl = pd.read_excel(\"data/stocks.xlsx\", sheet_name='msft')\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel files can be written using the .to_excel() method of DataFrame. Writing to the\n",
    "XLS format requires the inclusion of the XLWT package, so make sure it is loaded in your\n",
    "Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to an .XLS file, in worksheet 'Sheet1'\n",
    "df.to_excel(\"data/stocks2.xls\", sheet_name='MSFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write more than one DataFrame to a single Excel file and each DataFrame object on a\n",
    "separate worksheet, use the ExcelWriter object, along with the with keyword.\n",
    "ExcelWriter is part of pandas, but you will need to make sure it is imported, as it is not in\n",
    "the top level namespace of pandas. The following writes two DataFrame objects to two\n",
    "different worksheets in one Excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write multiple sheets\n",
    "# requires use of the ExcelWriter class\n",
    "from pandas import ExcelWriter\n",
    "with ExcelWriter(\"data/all_stocks.xls\") as writer:\n",
    "    aapl.to_excel(writer, sheet_name='AAPL')\n",
    "    df.to_excel(writer, sheet_name='MSFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to XLSX files uses the same function but specifies .XLSX through the file\n",
    "extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to xlsx\n",
    "df.to_excel(\"data/msft2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading and writing JSON files</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate saving as JSON, we will save the Excel data we just read in to a JSON file\n",
    "and then take a look at the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close    Volume  Adj Close\n",
       "0  7/21/2014  94.99  95.00  93.72  93.94  38887700      93.94\n",
       "1  7/18/2014  93.62  94.74  93.02  94.43  49898600      94.43\n",
       "2  7/17/2014  95.03  95.28  92.57  93.09  57152000      93.09\n",
       "3  7/16/2014  96.97  97.10  94.74  94.78  53396300      94.78\n",
       "4  7/15/2014  96.80  96.85  95.03  95.32  45477900      95.32"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"data/stocks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON-based data can be read with the pd.read_json() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close    Volume  Adj Close\n",
       "0 2014-07-21  94.99  95.00  93.72  93.94  38887700      93.94\n",
       "1 2014-07-18  93.62  94.74  93.02  94.43  49898600      94.43\n",
       "2 2014-07-17  95.03  95.28  92.57  93.09  57152000      93.09\n",
       "3 2014-07-16  96.97  97.10  94.74  94.78  53396300      94.78\n",
       "4 2014-07-15  96.80  96.85  95.03  95.32  45477900      95.32"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data in from JSON\n",
    "df_from_json = pd.read_json(\"data/stocks.json\")\n",
    "df_from_json.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading HTML data from the Web</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>pandas has very nice support for reading data from HTML files (or HTML from URLs).\n",
    "Underneath the covers, pandas makes use of the LXML, Html5Lib, and BeautifulSoup4\n",
    "packages, which provide some very impressive capabilities for reading and writing HTML\n",
    "tables.</p>\n",
    "<p>The pd.read_html() function will read HTML from a file (or URL) and parse all HTML\n",
    "tables found in the content into one or more pandas DataFrame object. The function\n",
    "always returns a list of DataFrame objects (actually, zero or more, depending on the\n",
    "number of tables found in the HTML).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                             Bank Name           City  ST   CERT  \\\n",
       " 0                 The First State Bank  Barboursville  WV  14361   \n",
       " 1                   Ericson State Bank        Ericson  NE  18265   \n",
       " 2     City National Bank of New Jersey         Newark  NJ  21111   \n",
       " 3                        Resolute Bank         Maumee  OH  58317   \n",
       " 4                Louisa Community Bank         Louisa  KY  58112   \n",
       " ..                                 ...            ...  ..    ...   \n",
       " 556                 Superior Bank, FSB       Hinsdale  IL  32646   \n",
       " 557                Malta National Bank          Malta  OH   6629   \n",
       " 558    First Alliance Bank & Trust Co.     Manchester  NH  34264   \n",
       " 559  National State Bank of Metropolis     Metropolis  IL   3815   \n",
       " 560                   Bank of Honolulu       Honolulu  HI  21029   \n",
       " \n",
       "                    Acquiring Institution       Closing Date  \n",
       " 0                         MVB Bank, Inc.      April 3, 2020  \n",
       " 1             Farmers and Merchants Bank  February 14, 2020  \n",
       " 2                        Industrial Bank   November 1, 2019  \n",
       " 3                     Buckeye State Bank   October 25, 2019  \n",
       " 4      Kentucky Farmers Bank Corporation   October 25, 2019  \n",
       " ..                                   ...                ...  \n",
       " 556                Superior Federal, FSB      July 27, 2001  \n",
       " 557                    North Valley Bank        May 3, 2001  \n",
       " 558  Southern New Hampshire Bank & Trust   February 2, 2001  \n",
       " 559              Banterra Bank of Marion  December 14, 2000  \n",
       " 560                   Bank of the Orient   October 13, 2000  \n",
       " \n",
       " [561 rows x 6 columns]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the URL to read\n",
    "url = \"http://www.fdic.gov/bank/individual/failed/banklist.html\"\n",
    "\n",
    "# read it\n",
    "banks = pd.read_html(url)\n",
    "\n",
    "# examine a subset of the first table read\n",
    "banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          Bank Name           City  ST   CERT  \\\n",
       "0              The First State Bank  Barboursville  WV  14361   \n",
       "1                Ericson State Bank        Ericson  NE  18265   \n",
       "2  City National Bank of New Jersey         Newark  NJ  21111   \n",
       "3                     Resolute Bank         Maumee  OH  58317   \n",
       "4             Louisa Community Bank         Louisa  KY  58112   \n",
       "\n",
       "               Acquiring Institution       Closing Date  \n",
       "0                     MVB Bank, Inc.      April 3, 2020  \n",
       "1         Farmers and Merchants Bank  February 14, 2020  \n",
       "2                    Industrial Bank   November 1, 2019  \n",
       "3                 Buckeye State Bank   October 25, 2019  \n",
       "4  Kentucky Farmers Bank Corporation   October 25, 2019  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banks[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close    Volume  Adj Close\n",
       "0  7/21/2014  94.99  95.00  93.72  93.94  38887700      93.94\n",
       "1  7/18/2014  93.62  94.74  93.02  94.43  49898600      94.43\n",
       "2  7/17/2014  95.03  95.28  92.57  93.09  57152000      93.09\n",
       "3  7/16/2014  96.97  97.10  94.74  94.78  53396300      94.78\n",
       "4  7/15/2014  96.80  96.85  95.03  95.32  45477900      95.32"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write DataFrame to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to HTML Document\n",
    "df.head(2).to_html(\"data/stocks.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading and writing HDF5 format files</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is a data model, library, and file format to store and manage data. It is commonly\n",
    "used in scientific computing environments. It supports an unlimited variety of data types\n",
    "and is designed for flexible and efficient I/O and for high volume and complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is portable and is extensible, allowing applications to evolve in their use of HDF5.\n",
    "The HDF5 Technology suite includes tools and applications to manage, manipulate, view,\n",
    "and analyze data in the HDF5 format. HDF5 is:\n",
    "\n",
    "<ul>\n",
    "<li>A versatile data model that can represent very complex data objects and a wide\n",
    "    variety of metadata</li>\n",
    "<li>A completely portable file format with no limit on the number or size of data objects\n",
    "in the collection</li>\n",
    "<li>A software library that runs on a range of computational platforms, from laptops to\n",
    "massively parallel systems, and implements a high-level API with C, C++, Fortran\n",
    "90, and Java interfaces</li>\n",
    "<li>A rich set of integrated performance features that allow for access time and storage\n",
    "space optimizations</li>\n",
    "<li>Tools and applications to manage, manipulate, view, and analyze the data in the\n",
    "collection    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>HDFStore is a hierarchical, dictionary-like object that reads and writes pandas objects to\n",
    "the HDF5 format. Under the covers, HDFStore uses the PyTables library, so make sure that\n",
    "it is installed if you want to use this format.</p>\n",
    "<p>The following demonstrates writing DataFrame into an HDF5 format. The output shows\n",
    "you that the HDF5 store has a root level object named df, which is a frame and whose\n",
    "shape is eight rows of three columns:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   A         B         C\n",
       "2000-01-01  0.469112 -0.282863 -1.509059\n",
       "2000-01-02 -1.135632  1.212112 -0.173215\n",
       "2000-01-03  0.119209 -1.044236 -0.861849\n",
       "2000-01-04 -2.104569 -0.494929  1.071804\n",
       "2000-01-05  0.721555 -0.706771 -1.039575\n",
       "2000-01-06  0.271860 -0.424972  0.567020\n",
       "2000-01-07  0.276232 -1.087401 -0.673690\n",
       "2000-01-08  0.113648 -1.478427  0.524988"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed for replication\n",
    "np.random.seed(123456)\n",
    "\n",
    "# create a DataFrame of dates and random numbers in three columns\n",
    "df = pd.DataFrame(np.random.randn(8, 3), index=pd.date_range('1/1/2000', periods=8),\n",
    "columns=['A', 'B', 'C'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.io.pytables.HDFStore'>\n",
       "File path: data/store.h5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create HDF5 store\n",
    "store = pd.HDFStore('data/store.h5')\n",
    "store['df'] = df # persisting happened here\n",
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following reads the HDF5 store and retrieves DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   A         B         C\n",
       "2000-01-01  0.469112 -0.282863 -1.509059\n",
       "2000-01-02 -1.135632  1.212112 -0.173215\n",
       "2000-01-03  0.119209 -1.044236 -0.861849\n",
       "2000-01-04 -2.104569 -0.494929  1.071804\n",
       "2000-01-05  0.721555 -0.706771 -1.039575\n",
       "2000-01-06  0.271860 -0.424972  0.567020\n",
       "2000-01-07  0.276232 -1.087401 -0.673690\n",
       "2000-01-08  0.113648 -1.478427  0.524988"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data from HDF5\n",
    "store = pd.HDFStore(\"data/store.h5\")\n",
    "df = store['df']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to DataFrame made after that point are not persisted, at least not until the object\n",
    "is assigned to the data store object again. The following demonstrates this by making a\n",
    "change to DataFrame and then reassigning it to the HDF5 store, thereby updating the data\n",
    "store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   A         B         C\n",
       "2000-01-01  1.000000 -0.282863 -1.509059\n",
       "2000-01-02  1.000000  1.212112 -0.173215\n",
       "2000-01-03  0.119209 -1.044236 -0.861849\n",
       "2000-01-04  1.000000 -0.494929  1.071804\n",
       "2000-01-05  0.721555 -0.706771 -1.039575\n",
       "2000-01-06  0.271860 -0.424972  0.567020\n",
       "2000-01-07  0.276232 -1.087401 -0.673690\n",
       "2000-01-08  0.113648 -1.478427  0.524988"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this changes the DataFrame, but did not persist\n",
    "df.loc['2000-01-01'].A = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to persist the change, assign the DataFrame to the\n",
    "# HDF5 store object\n",
    "store['df'] = df\n",
    "\n",
    "# it is now persisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   A         B         C\n",
       "2000-01-01  0.469112 -0.282863 -1.509059\n",
       "2000-01-02 -1.135632  1.212112 -0.173215"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following loads the store and\n",
    "# shows the first two rows, demonstrating\n",
    "# the the persisting was done\n",
    "pd.HDFStore(\"data/store.h5\")['df'].head(2) # it's now in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accessing data on the web and in the\n",
    "cloud</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite common to read data off the web and from the cloud. pandas makes it extremely\n",
    "easy to read data from the web and cloud. All of the pandas functions we have examined\n",
    "can also be given an HTTP URL, FTP address, or S3 address instead of a local file path,\n",
    "and all work just the same as they work with a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv directly from Yahoo! Finance from a URL\n",
    "df = pd.read_csv(\"http://ichart.yahoo.com/table.csv?s=MSFT&\" +\n",
    "\"a=5&b=1&c=2014&\" +\n",
    "\"d=5&e=30&f=2014&\" +\n",
    "\"g=d&ignore=.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading and writing from/to SQL databases</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>pandas can read data from any SQL databases that support Python data adapters, that\n",
    "respect the Python DB-API. Reading is performed using the pandas.io.sql.read_sql()\n",
    "function and writing to SQL databases using the .to_sql() method of DataFrame.</p>\n",
    "<p>As an example of writing, the following reads the stock data from msft.csv and\n",
    "aapl.csv. It then makes a connection to a SQLite3 database file. If the file does not exist,\n",
    "it creates it on the fly. It then writes the MSFT data to a table named STOCK_DATA. If the\n",
    "table did not exist, it is created. If it exists, all the data is replaced with the MSFT data. It\n",
    "then appends the AAPL stock data to that table:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference SQLite\n",
    "import sqlite3\n",
    "\n",
    "# read in the stock data from CSV\n",
    "msft = pd.read_csv(\"data/msft.csv\")\n",
    "msft[\"Symbol\"]=\"MSFT\"\n",
    "aapl = pd.read_csv(\"data/aapl.csv\")\n",
    "aapl[\"Symbol\"]=\"AAPL\"\n",
    "\n",
    "# create connection\n",
    "connection = sqlite3.connect(\"data/stocks.sqlite\")\n",
    "\n",
    "# .to_sql() will create SQL to store the DataFrame\n",
    "# in the specified table. if_exists specifies\n",
    "# what to do if the table already exists\n",
    "msft.to_sql(\"STOCK_DATA\", connection, if_exists=\"replace\")\n",
    "aapl.to_sql(\"STOCK_DATA\", connection, if_exists=\"append\")\n",
    "# commit the SQL and close the connection\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be read using SQL from the database using the pd.io.sql.read_sql() function.\n",
    "The following queries the data from stocks.sqlite using SQL and reports it to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database file\n",
    "connection = sqlite3.connect(\"data/stocks.sqlite\")\n",
    "\n",
    "# query all records in STOCK_DATA\n",
    "# returns a DataFrame\n",
    "# inde_col specifies which column to make the DataFrame index\n",
    "stocks = pd.io.sql.read_sql(\"SELECT * FROM STOCK_DATA;\",\n",
    "connection, index_col='index')\n",
    "\n",
    "# close the connection\n",
    "connection.close()\n",
    "\n",
    "# report the head of the data retrieved\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use the WHERE clause in the SQL, as well as to select columns. To\n",
    "demonstrate, the following selects the records where MSFTâ€™s volume is greater than\n",
    "29200100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the connection\n",
    "connection = sqlite3.connect(\"data/stocks.sqlite\")\n",
    "\n",
    "# construct the query string\n",
    "query = \"SELECT * FROM STOCK_DATA WHERE Volume>29200100 AND Symbol='MSFT';\"\n",
    "\n",
    "# execute and close connection\n",
    "items = pd.io.sql.read_sql(query, connection, index_col='index')\n",
    "\n",
    "connection.close()\n",
    "# report the query result\n",
    "\n",
    "Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final point, is that most of the code of these examples was SQLite3 code. The only\n",
    "pandas part of these examples is the use of the .to_sql() and .read_sql() methods. As\n",
    "these functions take a connection object, which can be any Python DB-API-ompatible data\n",
    "adapter, you can more or less work with any supported database data by simply creating\n",
    "an appropriate connection object. The code at the pandas level should remain the same for\n",
    "any supported database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading data from remote data services</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>pandas has direct support for various web-based data source classes in the\n",
    "pandas.io.data namespace. The primary class of interest is\n",
    "pandas.io.data.DataReader, which is implemented to read data from various supported\n",
    "sources and return it to the application directly as DataFrame.</p>\n",
    "<p>Currently, support exists for the following sources via the DataReader class:</p>\n",
    "<ul>\n",
    "    <li>Daily historical pricesâ€™ stock from either Yahoo! and Google Finance</li>\n",
    "    <li>Yahoo! Options</li>\n",
    "<li>The Federal Reserve Economic Data library</li>\n",
    "    <li>Kenneth Frenchâ€™s Data Library</li>\n",
    "<li>The World Bank</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific source of data is specified via the DataReader objectâ€™s data_source\n",
    "parameter. The specific items to be retrieved are specified using the name parameter. If the\n",
    "data source supports selecting data between a range of dates, these dates can be specified\n",
    "with the start and end parameters. We will now take a look at reading data from each of\n",
    "these sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading stock data from Yahoo! and Google\n",
    "Finance</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yahoo! Finance is specified by passing 'yahoo' as the data_source parameter. The\n",
    "following retrieves data from Yahoo! Finance, specifically, the data for MSFT between\n",
    "2012-01-01 and 2014-01-27:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 High        Low       Open      Close      Volume  Adj Close\n",
       "Date                                                                         \n",
       "2012-01-03  26.959999  26.389999  26.549999  26.770000  64731500.0  21.959635\n",
       "2012-01-04  27.469999  26.780001  26.820000  27.400000  80516100.0  22.476425\n",
       "2012-01-05  27.730000  27.290001  27.379999  27.680000  56081400.0  22.706108\n",
       "2012-01-06  28.190001  27.530001  27.530001  28.110001  99455500.0  23.058842\n",
       "2012-01-09  28.100000  27.719999  28.049999  27.740000  59706800.0  22.755325"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas.io.data namespace, alias as web\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# and datetime for the dates\n",
    "import datetime\n",
    "\n",
    "# start end end dates\n",
    "start = datetime.datetime(2012, 1, 1)\n",
    "end = datetime.datetime(2014, 1, 27)\n",
    "\n",
    "# read the MSFT stock data from yahoo! and view the head\n",
    "yahoo = web.DataReader('MSFT', 'yahoo', start, end)\n",
    "yahoo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of the data can be changed to Google Finance with a change of the\n",
    "data_source parameter to 'google':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "data_source='google' is not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-5356739eaa00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read from google and display the head of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgoog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSFT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'google'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgoog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pandas_datareader/data.py\u001b[0m in \u001b[0;36mDataReader\u001b[0;34m(name, data_source, start, end, retry_count, pause, session, api_key)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_source\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpected_source\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data_source=%r is not implemented\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_source\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"yahoo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: data_source='google' is not implemented"
     ]
    }
   ],
   "source": [
    "# read from google and display the head of the data\n",
    "goog = web.DataReader(\"MSFT\", 'google', start, end)\n",
    "goog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
